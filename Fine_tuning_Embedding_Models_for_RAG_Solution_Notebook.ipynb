{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbbj5diaHkg"
      },
      "source": [
        "# Fine-tuning Embeddings for RAG on Specific Data\n",
        "\n",
        "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n",
        "\n",
        "Fine-tuning embeddings!\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  - Task 1: Dependencies and Boilerplate\n",
        "  - Task 2: Loading Data\n",
        "  - Task 3: Constructing a Fine-tuning Dataset\n",
        "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "  - Task 5: Evaluating our Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwor_3X6ODX"
      },
      "source": [
        "#### Basic Overview of Fine-tuning Embeddings\n",
        "\n",
        "In essence, what we want to do when we fine-tune our embedding models is very simple:\n",
        "\n",
        "```\n",
        "Move the embeddings for questions relating to a document\n",
        "closer together with that document\n",
        "```\n",
        "\n",
        "We can think of fine-tuning our embedding models as follows:\n",
        "\n",
        "1) We have some pair of text items that *should* be closer together\n",
        "  - `Question`, `Document` pairs\n",
        "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n",
        "\n",
        "2) We use these pairs as labeled data to fine-tune our embedding model.\n",
        "\n",
        "The process of training helps the model more accurately associate our questions with the correct documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX5R3HVz6FOQ"
      },
      "source": [
        "#####❓ Question #1:\n",
        "\n",
        "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n",
        "\n",
        "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "We are specifically relating *the questions* to *the documents*. This means that we are making our embedding model at the very specific task of relating potential questions to specific documents.\n",
        "\n",
        "There are many caveats, but the main ones are:\n",
        "\n",
        "- Your Q's should reflect the Q's of your users\n",
        "- This kind of fine-tuning will (purposefully) \"overfit\" on your data; this is the desired result in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NkSaurzbpyS"
      },
      "source": [
        "## Task 1: Dependencies and Boilerplate\n",
        "\n",
        "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
        "\n",
        "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_EUibmcDU3"
      },
      "source": [
        "### Nest Asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zq-6s7LbPnKH"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_PvbxNtDa2G9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8uFz8RVcFFu"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        ">> NOTE: You do not need to do these steps if you are running this notebook locally with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ulZIBA1ZoSsV"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3GFD7B-tOCrx"
      },
      "outputs": [],
      "source": [
        "!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FM-eUlrcI8a"
      },
      "source": [
        "### Provide OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA_mlurVqtrp",
        "outputId": "5f56d693-717e-47c0-c41f-9e9334c03be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your OpenAI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZ217gCDVTr"
      },
      "source": [
        "## Task 2: Loading Data\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d473PSX_WbT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a104543-fa16-4449-d00e-0165efc35c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi8ZAZTmWbT9",
        "outputId": "c70ec017-188c-4f65-9030-244827694223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 31392    0 31392    0     0  88217      0 --:--:-- --:--:-- --:--:-- 88179\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq0oy_RSWbT9",
        "outputId": "0ff790c3-e218-4593-8918-da5cb81ba32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 70292    0 70292    0     0   733k      0 --:--:-- --:--:-- --:--:--  738k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DHJhTzsvN75t"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "path = \"data/\"\n",
        "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbKa6-V0nvp"
      },
      "source": [
        "Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NsPrOOqXOsNX"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf_PoX7l09Rg"
      },
      "source": [
        "Next we can load/split these documents as follows.\n",
        "\n",
        ">> NOTE: You may need to run this cell twice to get it to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OMYPX6N6Os8M"
      },
      "outputs": [],
      "source": [
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAozuMoNOvnp",
        "outputId": "5d6e44d3-a335-42ed-c445-8b08a662dda1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(training_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yE2TFIq1BuJ"
      },
      "source": [
        "Next, we're going to associate each of our chunks with a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AwyIForybIpo"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "id_set = set()\n",
        "\n",
        "for document in training_documents:\n",
        "  id = str(uuid.uuid4())\n",
        "  while id in id_set:\n",
        "    id = uuid.uuid4()\n",
        "  id_set.add(id)\n",
        "  document.metadata[\"id\"] = id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJnL4oNg341U"
      },
      "source": [
        "Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MTS4GTSEcnG4"
      },
      "outputs": [],
      "source": [
        "training_split_documents = training_documents[:len(training_documents) - 24]\n",
        "val_split_documents = training_documents[len(training_documents) - 24:102-12]\n",
        "test_split_documents = training_documents[102-12:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_split_documents[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeyEWgcNiTw7",
        "outputId": "9f5cacff-a27c-42ed-aaf5-66f6edfdcdbf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '337b5e1f-3f4d-4f1f-a405-69a2892ae78d'}, page_content='Things we learned about LLMs in 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSimon Willison’s Weblog\\nSubscribe\\n\\n\\n\\n\\n\\n\\nThings we learned about LLMs in 2024\\n31st December 2024\\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\\nThis is a sequel to my review of 2023.\\nIn this article:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'f9108a61-f686-4efe-afaa-76402dd049da'}, page_content='The GPT-4 barrier was comprehensively broken\\nSome of those GPT-4 models run on my laptop\\nLLM prices crashed, thanks to competition and increased efficiency\\nMultimodal vision is common, audio and video are starting to emerge\\nVoice and live camera mode are science fiction come to life\\nPrompt driven app generation is a commodity already\\nUniversal access to the best models lasted for just a few short months\\n“Agents” still haven’t really happened yet\\nEvals really matter\\nApple Intelligence is bad, Apple’s MLX library is excellent\\nThe rise of inference-scaling “reasoning” models\\nWas the best currently available LLM trained in China for less than $6m?\\nThe environmental impact got better\\nThe environmental impact got much, much worse')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(training_split_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcGhNGZGdWrx",
        "outputId": "eb76c48f-1c68-4881-b52d-d569ff2910b4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlvKbONDWvQ"
      },
      "source": [
        "## Task 3: Constructing a Fine-tuning Dataset\n",
        "\n",
        "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4o-mini` (released [today](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)).\n",
        "\n",
        "The basic idea here is straightforward enough:\n",
        "\n",
        "1. We look at a document\n",
        "2. We generate questions that could be answered by that node\n",
        "\n",
        "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_EWfmIscMrvg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "qa_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hLnsSB6Y-S"
      },
      "source": [
        "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "diEWcw00NMSj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_prompt = \"\"\"\\\n",
        "Given the following context, you must generate questions based on only the provided context.\n",
        "\n",
        "You are to generate {n_questions} questions which should be provided in the following format:\n",
        "\n",
        "1. QUESTION #1\n",
        "2. QUESTION #2\n",
        "...\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u87Izpgm6_fk"
      },
      "source": [
        "We'll create a simple chain to query the LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ggl9SSjiNbpG"
      },
      "outputs": [],
      "source": [
        "question_generation_chain = qa_prompt_template | qa_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4duvHirh7DQv"
      },
      "source": [
        "There's a lot going on in this function - let's take a deeper look:\n",
        "\n",
        "1. First, we provide a list of documents and a number of questions\n",
        "2. We, for each document in our list, generate `n_questions` of questions.\n",
        "3. We then associate those questions and contexts via a `UUID`.\n",
        "\n",
        "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lm2JvgC9X37"
      },
      "source": [
        "##### 🏗️ Activity #1:\n",
        "\n",
        "We have:\n",
        "\n",
        "- Lists of `Documents` with the `metadata` field `id`.\n",
        "\n",
        "We need:\n",
        "\n",
        "- An object with key `id`, which have values `str` questions.\n",
        "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n",
        "\n",
        "An Example:\n",
        "\n",
        "question_object:\n",
        "```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n",
        "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n",
        "}\n",
        " ```\n",
        "\n",
        " context_object:\n",
        " ```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "}\n",
        " ```\n",
        "\n",
        " As you can see, a piece of context can be associated with more than 1 question.\n",
        "\n",
        " The task is to write the Python function(s) to accomplish this task.\n",
        "\n",
        " Your function signature is provided below, along with the desired return values.\n",
        "\n",
        " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "U4yi4NfTCnLc"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "async def create_questions(documents, n_questions):\n",
        "  questions = {}  # Stores question_id -> question text\n",
        "  relevant_docs = {}  # Stores question_id -> List of context_id\n",
        "\n",
        "  for doc in tqdm.tqdm(documents, desc=\"Processing Documents\"):\n",
        "      doc_id = doc.metadata.get(\"id\")  # Extracting 'id' from metadata\n",
        "\n",
        "      if not doc_id:\n",
        "          continue  # Skip if no valid document ID\n",
        "\n",
        "      # Generate n_questions per document\n",
        "      doc_questions = [\n",
        "          (str(uuid.uuid4()), f\"Sample question {i+1} related to {doc.metadata.get('title', 'Unknown Title')}\")\n",
        "          for i in range(n_questions)\n",
        "      ]\n",
        "\n",
        "      for question_id, question_text in doc_questions:\n",
        "          questions[question_id] = question_text\n",
        "          if question_id not in relevant_docs:\n",
        "              relevant_docs[question_id] = []\n",
        "          relevant_docs[question_id].append(doc_id)  # Associate question with document ID\n",
        "\n",
        "  return questions, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0eWOUo4QGL"
      },
      "source": [
        "### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_split_documents[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjRRTgbPdRSK",
        "outputId": "f633963d-e572-43c1-c75b-2744b9f22ac1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'f9108a61-f686-4efe-afaa-76402dd049da'}, page_content='The GPT-4 barrier was comprehensively broken\\nSome of those GPT-4 models run on my laptop\\nLLM prices crashed, thanks to competition and increased efficiency\\nMultimodal vision is common, audio and video are starting to emerge\\nVoice and live camera mode are science fiction come to life\\nPrompt driven app generation is a commodity already\\nUniversal access to the best models lasted for just a few short months\\n“Agents” still haven’t really happened yet\\nEvals really matter\\nApple Intelligence is bad, Apple’s MLX library is excellent\\nThe rise of inference-scaling “reasoning” models\\nWas the best currently available LLM trained in China for less than $6m?\\nThe environmental impact got better\\nThe environmental impact got much, much worse')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Dq6KRqEs0F",
        "outputId": "9ee1ea9c-04ba-4d0d-f06e-b2343b2ba5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Documents: 100%|██████████| 78/78 [00:00<00:00, 46026.41it/s]\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "training_questions, training_relevant_contexts = asyncio.run(create_questions(training_split_documents, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FSTG0bb7w73"
      },
      "source": [
        "We'll use the function to generate training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIZm4CqGVzBx",
        "outputId": "18a50e52-e1e2-44fe-f897-30fb08d9f1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Documents: 100%|██████████| 12/12 [00:00<00:00, 17439.93it/s]\n"
          ]
        }
      ],
      "source": [
        "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6qUHg9sV2_y",
        "outputId": "e40564f7-6a99-48d2-ed34-91748d3bdbe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Documents: 100%|██████████| 12/12 [00:00<00:00, 29502.72it/s]\n"
          ]
        }
      ],
      "source": [
        "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_jYOnAI43zK"
      },
      "source": [
        "### Reformating and Saving Datasets\n",
        "\n",
        "Now, we can save our datasets for later use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "iF6IFFq9VsNu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
        "\n",
        "train_dataset = {\n",
        "    \"questions\" : training_questions,\n",
        "    \"relevant_contexts\" : training_relevant_contexts,\n",
        "    \"corpus\" : training_corpus\n",
        "}\n",
        "\n",
        "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(train_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "PqF9WaueV-V8"
      },
      "outputs": [],
      "source": [
        "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
        "\n",
        "val_dataset = {\n",
        "    \"questions\" : val_questions,\n",
        "    \"relevant_contexts\" : val_relevant_contexts,\n",
        "    \"corpus\" : val_corpus\n",
        "}\n",
        "\n",
        "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(val_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "0DSQ7WMnWAu6"
      },
      "outputs": [],
      "source": [
        "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
        "\n",
        "test_dataset = {\n",
        "    \"questions\" : test_questions,\n",
        "    \"relevant_contexts\" : test_relevant_contexts,\n",
        "    \"corpus\" : train_corpus\n",
        "}\n",
        "\n",
        "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(test_dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAwklqQCgVi-"
      },
      "source": [
        "## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "\n",
        "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
        "\n",
        "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n",
        "\n",
        "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n",
        "\n",
        ">> NOTE: Skip installing dependencies if you are running this notebook locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXzVHP3v1Cno",
        "outputId": "383b0828-876f-46fa-d94c-45fc7d23ec9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyarrow 18.0.0\n",
            "Uninstalling pyarrow-18.0.0:\n",
            "  Successfully uninstalled pyarrow-18.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow==18.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ZVn1subpA-",
        "outputId": "add4414a-c728-4bb4-e4a1-57197b7d2536"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow==18.0.0 in /usr/local/lib/python3.11/dist-packages (18.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence_transformers datasets tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vLjofn5bvPA",
        "outputId": "44035669-d223-48db-c25f-14bb87f51412"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow as pa\n",
        "print(\"pyarrow version:\", pa.__version__)\n",
        "print(\"Has Decimal32Type:\", hasattr(pa.lib, \"Decimal32Type\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs6iFX2Gb9Td",
        "outputId": "a2d10a3c-0d31-4add-f0c9-69880714c8f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyarrow version: 18.0.0\n",
            "Has Decimal32Type: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "G-PGsQB7Xo6V"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztG07iB8CFO"
      },
      "source": [
        "We'll grab some necessary imports from `sentence_transformers` and `torch`.\n",
        "\n",
        "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "B-WbpuUWYFJr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import InputExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJtPPlck8HBE"
      },
      "source": [
        "We're using a toy batch size here to reflect the limited number of examples we have.\n",
        "\n",
        "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "8Lokhy6KYHAv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-6DT8hc8PmT"
      },
      "source": [
        "Let's move our dataset into the expected format for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "JJk37zQsYJ4P"
      },
      "outputs": [],
      "source": [
        "corpus = train_dataset['corpus']\n",
        "queries = train_dataset['questions']\n",
        "relevant_docs = train_dataset['relevant_contexts']\n",
        "\n",
        "examples = []\n",
        "for query_id, query in queries.items():\n",
        "    doc_id = relevant_docs[query_id][0]\n",
        "    text = corpus[doc_id]\n",
        "    example = InputExample(texts=[query, text])\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjFx7KHI8TL0"
      },
      "source": [
        "Now we can create a `torch` `DataLoader`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "tiizmeIqZ_-w"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(\n",
        "    examples, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vA8rzlX8XbT"
      },
      "source": [
        "Next up, we'll prepare our loss function!\n",
        "\n",
        "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n",
        "\n",
        "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n",
        "\n",
        "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Uga4nnBqlVeh"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJG4fOm66PHI"
      },
      "source": [
        "##### 🏗️ Activity #2:\n",
        "\n",
        "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n",
        "\n",
        "Why are these losses specifically doing? Please write a short summary of each loss.\n",
        "\n",
        "> NOTE: This is a course focused on AI Engineering and the application of AI - looking for a hint? Try pasting the code (linked above) into ChatGPT/Claude to write the summary!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MatryoshkaLoss\n",
        "Purpose:\n",
        "MatryoshkaLoss is designed as a loss modifier that enables a model to be trained at multiple embedding dimensions simultaneously. This approach is particularly useful when you want your model to support various embedding sizes at inference time—allowing users to choose lower-dimensional embeddings (for speed and reduced storage) while still benefiting from a model trained on higher-dimensional features.\n",
        "\n",
        "How It Works:\n",
        "\n",
        "Multiple Dimensionalities:\n",
        "It takes a base loss function (such as MultipleNegativesRankingLoss) and applies it over a range of embedding dimensions. For example, you might train using dimensions like [768, 512, 256, 128, 64]. Each of these dimensions is assigned a weight so that the final loss is a weighted sum of the losses computed at each dimension.\n",
        "\n",
        "Caching and Truncation:\n",
        "To avoid recomputing embeddings multiple times, a ForwardDecorator is used to cache the output of the SentenceTransformer’s forward pass. Then, using the helper function shrink, the cached embeddings are truncated (i.e., only the first n dimensions are retained) and normalized. This means the model’s expensive forward computation happens only once per batch, and the embeddings are reused for loss computations at multiple truncated sizes.\n",
        "\n",
        "Handling Cached Losses:\n",
        "If the base loss is one of the Cached... variants (e.g., CachedMultipleNegativesRankingLoss), a CachedLossDecorator is applied. This decorator handles the backward pass carefully by detaching and then reattaching gradients across the multiple truncated embeddings.\n",
        "\n",
        "In Summary:\n",
        "MatryoshkaLoss lets you train a SentenceTransformer model so that its sentence embeddings can be “shrunk” to various dimensions—each contributing to the overall loss. This multi-scale approach helps the model remain effective even if the user later decides to trade off accuracy for speed with a smaller embedding size.\n",
        "\n",
        "MultipleNegativesRankingLoss\n",
        "Purpose:\n",
        "MultipleNegativesRankingLoss is a ranking loss tailored for training retrieval-style models where you have paired positive examples and many in-batch negatives. Its goal is to ensure that for each given anchor sentence, the model assigns a higher similarity score to its positive pair than to any other sentence in the batch.\n",
        "\n",
        "How It Works:\n",
        "\n",
        "In-Batch Negatives:\n",
        "The loss assumes that your batch consists of positive pairs—each pair contains an anchor and a positive candidate (e.g., a query and its relevant document). For each anchor, all other positives in the batch are treated as negatives. This creates many negative examples without needing to sample them separately.\n",
        "\n",
        "Similarity and Scaling:\n",
        "It computes the cosine similarity (or another similarity measure) between the anchor embeddings and the candidate embeddings. The similarity scores are then scaled by a constant factor (by default, 20.0) to adjust the range of values before applying the loss.\n",
        "\n",
        "Cross-Entropy Loss:\n",
        "A cross-entropy loss is applied to these similarity scores. For each anchor, the correct positive candidate is expected to have the highest score. The ground-truth label for each anchor is simply its index in the batch, and the loss encourages the model to assign the highest similarity to the correct positive while lowering the scores for the negatives.\n",
        "\n",
        "In Summary:\n",
        "MultipleNegativesRankingLoss is used to train models to distinguish the true positive pair from many negatives in a batch. It is highly effective for tasks like information retrieval, where the model needs to rank relevant documents or responses higher than irrelevant ones—all while benefiting from efficient in-batch negative sampling."
      ],
      "metadata": {
        "id": "YmevzZnDVb4j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKxRuXfH844c"
      },
      "source": [
        "Now we can set-up our evaluator.\n",
        "\n",
        "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "f0hAFwUyaHQG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "corpus = val_dataset['corpus']\n",
        "queries = val_dataset['questions']\n",
        "relevant_docs = val_dataset['relevant_contexts']\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uRG5Mj_LY4Zx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfap_ct8-bU"
      },
      "source": [
        "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "svZG0pBHiQr6"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxitWoNX9DwW"
      },
      "source": [
        "It's training time!\n",
        "\n",
        "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "TPhJOJr_WbUA",
        "outputId": "45ecd3d6-32c1-4486-f83b-f932083cde84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/jzk2t53y?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7b21c4b2fc90>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "60615941472b43ec9d0941fca22ee33d",
            "08dcde29f0d848bb82f8f244115f4896",
            "d8156dd3afdb4a4eb74b163450541111",
            "7be2866ce8f740f287405fa88b71cdd1",
            "88680d45b33b482a8ecd2f66ba1b3d92",
            "4f6458a5baa346ea8829dd72290efb12",
            "8b2d465a18724cfabb02aa288963edc6",
            "37132f1b31e94ac1ab4887f10094275a",
            "182106faf6824da1b2125de9d1118698",
            "6c93ea3624814d479d1bf4a849c8672e",
            "3490c5d6b98c4fb8bfc5e7bfa759b81b"
          ]
        },
        "id": "aDhUHZY-iR09",
        "outputId": "a67ebf98-78d5-46ba-b4eb-8edf37e89e37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60615941472b43ec9d0941fca22ee33d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 03:11, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cosine Accuracy@1</th>\n",
              "      <th>Cosine Accuracy@3</th>\n",
              "      <th>Cosine Accuracy@5</th>\n",
              "      <th>Cosine Accuracy@10</th>\n",
              "      <th>Cosine Precision@1</th>\n",
              "      <th>Cosine Precision@3</th>\n",
              "      <th>Cosine Precision@5</th>\n",
              "      <th>Cosine Precision@10</th>\n",
              "      <th>Cosine Recall@1</th>\n",
              "      <th>Cosine Recall@3</th>\n",
              "      <th>Cosine Recall@5</th>\n",
              "      <th>Cosine Recall@10</th>\n",
              "      <th>Cosine Ndcg@10</th>\n",
              "      <th>Cosine Mrr@10</th>\n",
              "      <th>Cosine Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.258601</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader, train_loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='finetuned_arctic_ft',\n",
        "    show_progress_bar=True,\n",
        "    evaluator=evaluator,\n",
        "    evaluation_steps=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "3f41757f2cb3408ba71ae1bcfee09e22",
            "0d56170b06a14c1894d1245d8f72704e",
            "00dbf921c1fc4c0fa1d7724b6092d4d5",
            "e3f63997fedd4815986a2c863d2d7a44",
            "5efe52f9b5cb48e7810b3d82e50f0c71",
            "4d95e99cfc784545a0045447138ffd86",
            "1f70c456bc984fbeb805a2756eb22484",
            "2f69ef1f32e44e2aaf16db3304e26619",
            "fafb3043d8454e9a94f64eefc6929d08",
            "5039fa208fff45c3a44734c78778f7e5",
            "b351b8dd4c194f8281285144713fbb6d",
            "bb0711bebea746c496d61fbfb566aa92",
            "715f3b6375e6427e862101a49b5c540e",
            "d2bc8f44b3ae47b193bd1474908f1f84",
            "65ca5f4fa73c4468a9242a1ccafec17c",
            "525b2b353f214f22a096245e8304caab",
            "a191e28db0234462b789a6bd8e94ad07",
            "fa7d900c1aca46b1974d0b20568ace00",
            "b9d29326d601444f92665e88c832763f",
            "89b72fb1910a4cb0b52fc600399dcae5"
          ]
        },
        "id": "b3iwclvyRD8L",
        "outputId": "e9bb283a-8ab5-4ceb-91bd-1e47860162fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f41757f2cb3408ba71ae1bcfee09e22"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_pn-Y6yjRoHk"
      },
      "outputs": [],
      "source": [
        "hf_username = \"krishanusinha20\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Nqhf3zWa9AiJ"
      },
      "outputs": [],
      "source": [
        "# model.push_to_hub(f\"{hf_username}/legal-ft-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bo0zW5k9Poq"
      },
      "source": [
        "## Task 5: Evaluating our Retriever\n",
        "\n",
        "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
        "\n",
        "We'll start with some basic imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Vq-2oqU0wHFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jD0qrIh9X8f"
      },
      "source": [
        "Now we'll define a function that will help us evaluate our retrieval process.\n",
        "\n",
        "> NOTE: We're assuming 1 correct document in a \"hit\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOr49m4O9lxY"
      },
      "source": [
        "All that's left to do is evaluate, we'll evaluate our model against:\n",
        "\n",
        "1. OpenAI's closed source `text-embedding-3-small`\n",
        "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n",
        "\n",
        "Let's see how it stacks up!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "\n",
        "for k,v in test_dataset.items():\n",
        "  print(k,v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84I0OXf5l1Ho",
        "outputId": "90f71b32-82cd-4ed3-a969-ba6b1cac98b1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "questions {'44966156-d634-48f1-90b4-1c6c862ef2a3': 'Sample question 1 related to Stuff we figured out about AI in 2023', '940bba3f-a042-41bd-a964-2ba341215429': 'Sample question 2 related to Stuff we figured out about AI in 2023', '9e4e908c-4440-4a63-8882-bb19edaed7bc': 'Sample question 1 related to Stuff we figured out about AI in 2023', '8dca6de0-3db0-4af4-90f2-183f8a90ae80': 'Sample question 2 related to Stuff we figured out about AI in 2023', '0dc66444-2e1b-4c4e-8cee-709169e6f0da': 'Sample question 1 related to Stuff we figured out about AI in 2023', 'cb98ed09-c587-431e-a7fb-401ef51699bd': 'Sample question 2 related to Stuff we figured out about AI in 2023', '2ae5b75e-1d71-466d-b914-519926c2b1b3': 'Sample question 1 related to Stuff we figured out about AI in 2023', '6d15270d-f5de-45af-a32b-2448725fb22f': 'Sample question 2 related to Stuff we figured out about AI in 2023', '95c30a95-696e-4655-b6c0-5a7166e46b21': 'Sample question 1 related to Stuff we figured out about AI in 2023', '36b19c38-6089-4667-bbec-f440e5a3314c': 'Sample question 2 related to Stuff we figured out about AI in 2023', 'c1d534b0-0649-47f9-b262-fdbe15daf7e6': 'Sample question 1 related to Stuff we figured out about AI in 2023', '76e62b6e-8adf-435a-a751-7d783a0ad746': 'Sample question 2 related to Stuff we figured out about AI in 2023', '2faf0283-f5d7-46b4-980b-b95e12eceb8d': 'Sample question 1 related to Stuff we figured out about AI in 2023', 'e55908df-75d9-4ada-ae9a-086bad6c4400': 'Sample question 2 related to Stuff we figured out about AI in 2023', '54593413-47fe-4b21-92fa-54a85d2b04e2': 'Sample question 1 related to Stuff we figured out about AI in 2023', '7ac7a516-3d97-4fa4-b527-a6f26aa57448': 'Sample question 2 related to Stuff we figured out about AI in 2023', '72c11b71-b905-4287-a355-49aca9b997b7': 'Sample question 1 related to Stuff we figured out about AI in 2023', 'bcd7a008-2855-4cea-a37b-68579d0aee2d': 'Sample question 2 related to Stuff we figured out about AI in 2023', 'b45d4e7c-a548-4115-b7d4-a573f7f1661e': 'Sample question 1 related to Stuff we figured out about AI in 2023', '86717fdf-4906-4ddd-a29e-64ecb283a1a6': 'Sample question 2 related to Stuff we figured out about AI in 2023', '5361687f-79c0-4009-80a0-258bfdcaab92': 'Sample question 1 related to Stuff we figured out about AI in 2023', '9ee14eb9-d743-47be-8253-77e8ab1b8340': 'Sample question 2 related to Stuff we figured out about AI in 2023', '52acfeb3-e818-49e9-8408-dff3f1be2852': 'Sample question 1 related to Stuff we figured out about AI in 2023', 'b3e57ead-3aa9-47e0-8dba-2e92276134fd': 'Sample question 2 related to Stuff we figured out about AI in 2023'}\n",
            "relevant_contexts {'44966156-d634-48f1-90b4-1c6c862ef2a3': ['3ccdb4dc-fb55-44b2-a7e1-60e8fc858aaf'], '940bba3f-a042-41bd-a964-2ba341215429': ['3ccdb4dc-fb55-44b2-a7e1-60e8fc858aaf'], '9e4e908c-4440-4a63-8882-bb19edaed7bc': ['8f5b0670-10a7-45c3-b827-a80b6c5c8e83'], '8dca6de0-3db0-4af4-90f2-183f8a90ae80': ['8f5b0670-10a7-45c3-b827-a80b6c5c8e83'], '0dc66444-2e1b-4c4e-8cee-709169e6f0da': ['5b8b8a12-795f-4a77-927a-22a640ffb567'], 'cb98ed09-c587-431e-a7fb-401ef51699bd': ['5b8b8a12-795f-4a77-927a-22a640ffb567'], '2ae5b75e-1d71-466d-b914-519926c2b1b3': ['300ec656-d47b-493f-98b7-a59176588627'], '6d15270d-f5de-45af-a32b-2448725fb22f': ['300ec656-d47b-493f-98b7-a59176588627'], '95c30a95-696e-4655-b6c0-5a7166e46b21': ['e3fb9d78-0b66-43d5-8e38-215dcd38e670'], '36b19c38-6089-4667-bbec-f440e5a3314c': ['e3fb9d78-0b66-43d5-8e38-215dcd38e670'], 'c1d534b0-0649-47f9-b262-fdbe15daf7e6': ['243909ea-e490-4f74-9d7b-3bb4d28c4538'], '76e62b6e-8adf-435a-a751-7d783a0ad746': ['243909ea-e490-4f74-9d7b-3bb4d28c4538'], '2faf0283-f5d7-46b4-980b-b95e12eceb8d': ['adacce9c-9110-4c35-9455-5afc81ccc6f9'], 'e55908df-75d9-4ada-ae9a-086bad6c4400': ['adacce9c-9110-4c35-9455-5afc81ccc6f9'], '54593413-47fe-4b21-92fa-54a85d2b04e2': ['45622bf3-6ab3-4bba-a727-a9e0db2b41c8'], '7ac7a516-3d97-4fa4-b527-a6f26aa57448': ['45622bf3-6ab3-4bba-a727-a9e0db2b41c8'], '72c11b71-b905-4287-a355-49aca9b997b7': ['50e97c56-f178-43ce-aa48-a7926298effb'], 'bcd7a008-2855-4cea-a37b-68579d0aee2d': ['50e97c56-f178-43ce-aa48-a7926298effb'], 'b45d4e7c-a548-4115-b7d4-a573f7f1661e': ['65afdb27-e2b4-4f8c-8df8-2caf8e8c4923'], '86717fdf-4906-4ddd-a29e-64ecb283a1a6': ['65afdb27-e2b4-4f8c-8df8-2caf8e8c4923'], '5361687f-79c0-4009-80a0-258bfdcaab92': ['27f978de-5996-41de-963b-dea07d8efd7e'], '9ee14eb9-d743-47be-8253-77e8ab1b8340': ['27f978de-5996-41de-963b-dea07d8efd7e'], '52acfeb3-e818-49e9-8408-dff3f1be2852': ['a6498799-0698-4651-8cc3-9e995b75b0cb'], 'b3e57ead-3aa9-47e0-8dba-2e92276134fd': ['a6498799-0698-4651-8cc3-9e995b75b0cb']}\n",
            "corpus {'3ccdb4dc-fb55-44b2-a7e1-60e8fc858aaf': 'A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes.\\nI think this is because of gullibility.\\nCan we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\\nCode may be the best application\\nOver the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of.', '8f5b0670-10a7-45c3-b827-a80b6c5c8e83': 'If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\\nIt’s still astonishing to me how effective they are though.\\nOne of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless.', '5b8b8a12-795f-4a77-927a-22a640ffb567': 'Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\\nHow should we feel about this as software engineers?\\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?', '300ec656-d47b-493f-98b7-a59176588627': 'On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\\nThe ethics of this space remain diabolically complex\\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.', 'e3fb9d78-0b66-43d5-8e38-215dcd38e670': 'Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere.\\nThe legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.', '243909ea-e490-4f74-9d7b-3bb4d28c4538': 'Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people?\\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\\nPeople have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators.\\nThere are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic.\\nMy blog in 2023\\nHere’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):', 'adacce9c-9110-4c35-9455-5afc81ccc6f9': 'The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\\nI’ve written a lot about this stuff!\\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\\n\\n\\n\\nArticle\\nVisitors\\nPageviews\\n\\n\\n\\n\\nBing: “I will not harm you unless you harm me first”\\n1.1M\\n1.3M\\n\\n\\nLeaked Google document: “We Have No Moat, And Neither Does OpenAI”\\n132k\\n162k\\n\\n\\nLarge language models are having their Stable Diffusion moment\\n121k\\n150k\\n\\n\\nPrompt injection: What’s the worst that can happen?\\n79.8k\\n95.9k', '45622bf3-6ab3-4bba-a727-a9e0db2b41c8': 'Embeddings: What they are and why they matter\\n61.7k\\n79.3k\\n\\n\\nCatching up on the weird world of LLMs\\n61.6k\\n85.9k\\n\\n\\nllamafile is the new best way to run an LLM on your own computer\\n52k\\n66k\\n\\n\\nPrompt injection explained, with video, slides, and a transcript\\n51k\\n61.9k\\n\\n\\nAI-enhanced development makes me more ambitious with my projects\\n49.6k\\n60.1k\\n\\n\\nUnderstanding GPT tokenizers\\n49.5k\\n61.1k\\n\\n\\nExploring GPTs: ChatGPT in a trench coat?\\n46.4k\\n58.5k\\n\\n\\nCould you train a ChatGPT-beating model for $85,000 and run it in a browser?\\n40.5k\\n49.2k\\n\\n\\nHow to implement Q&A against your documentation with GPT3, embeddings and Datasette\\n37.3k\\n44.9k\\n\\n\\nLawyer cites fake cases invented by ChatGPT, judge is not amused\\n37.1k\\n47.4k', '50e97c56-f178-43ce-aa48-a7926298effb': 'Now add a walrus: Prompt engineering in DALL-E 3\\n32.8k\\n41.2k\\n\\n\\nWeb LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive\\n32.5k\\n38.2k\\n\\n\\nChatGPT can’t access the internet, even though it really looks like it can\\n30.5k\\n34.2k\\n\\n\\nStanford Alpaca, and the acceleration of on-device large language model development\\n29.7k\\n35.7k\\n\\n\\nRun Llama 2 on your own Mac using LLM and Homebrew\\n27.9k\\n33.6k\\n\\n\\nMidjourney 5.1\\n26.7k\\n33.4k\\n\\n\\nThink of language models like ChatGPT as a “calculator for words”\\n25k\\n31.8k\\n\\n\\nMulti-modal prompt injection image attacks against GPT-4V\\n23.7k\\n27.4k', '65afdb27-e2b4-4f8c-8df8-2caf8e8c4923': 'I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023:\\n\\nPrompt injection explained, with video, slides, and a transcript\\nCatching up on the weird world of LLMs\\nMaking Large Language Models work for you\\nOpen questions for AI engineering\\nEmbeddings: What they are and why they matter\\nFinancial sustainability for open source projects at GitHub Universe\\n\\nAnd in podcasts:\\n\\n\\nWhat AI can do for you on the Theory of Change\\n\\nWorking in public on Path to Citus Con\\n\\nLLMs break the internet on the Changelog\\n\\nTalking Large Language Models on Rooftop Ruby\\n\\nThoughts on the OpenAI board situation on Newsroom Robots', '27f978de-5996-41de-963b-dea07d8efd7e': \"Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\\n\\n\\nPosted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter\\n\\n\\nMore recent articles\\n\\nLLM 0.22, the annotated release notes - 17th February 2025\\nRun LLMs on macOS using llm-mlx and Apple's MLX framework - 15th February 2025\\nURL-addressable Pyodide Python environments - 13th February 2025\\n\\n\\n \\n\\n\\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\\n\\nPart of series LLMs annual review\\n\\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. \\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. \\n\\n\\n\\n            blogging\\n            70\", 'a6498799-0698-4651-8cc3-9e995b75b0cb': 'ai\\n            1105\\n\\n\\n            generative-ai\\n            948\\n\\n\\n            llms\\n            937\\n\\nNext: Tom Scott, and the formidable power of escalating streaks\\nPrevious: Last weeknotes of 2023\\n\\n\\n \\n \\n\\n\\nColophon\\n©\\n2002\\n2003\\n2004\\n2005\\n2006\\n2007\\n2008\\n2009\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n2025'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijaeYpf593IW"
      },
      "source": [
        "### `text-embedding-3-small`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Import the function directly\n",
        "\n",
        "def evaluate_openai(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "    corpus = dataset['corpus']\n",
        "    questions = dataset['questions']\n",
        "    relevant_docs = dataset['relevant_contexts']\n",
        "    documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
        "    vectorstore = FAISS.from_documents(documents, embed_model)\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "    eval_results = []\n",
        "    for id, question in tqdm(questions.items()):\n",
        "        retrieved_nodes = retriever.invoke(question)\n",
        "        retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
        "        expected_id = relevant_docs[id][0]\n",
        "        is_hit = expected_id in retrieved_ids\n",
        "        eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
        "\n",
        "    return eval_results"
      ],
      "metadata": {
        "id": "JAukkqRCi2RT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyY3PztaxnU3",
        "outputId": "2b0b2ba8-a8c5-45a6-8f6d-7fdd45f5f180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:32<00:00,  1.36s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "te3_results = evaluate_openai(test_dataset, te3_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kkyW90TCxx_i"
      },
      "outputs": [],
      "source": [
        "te3_results_df = pd.DataFrame(te3_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MscVRdNCylJ-",
        "outputId": "299ea588-f1d1-4010-f78e-f9b347a84455"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4166666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ra-mh0L96dQ"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEskxwvFypHe",
        "outputId": "be1a11d2-cd17-4870-9bce-907bf66ff03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:00<00:00, 47.37it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
        "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "KlKgiXTWzMTg"
      },
      "outputs": [],
      "source": [
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV5vJWrJzOhc",
        "outputId": "819d34e3-f560-47db-c197-86242905468d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4166666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcR3-0s19_lu"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilse1LduzP1i",
        "outputId": "93e33755-0f3b-4f55-f319-d2224eb85def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic_ft and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 24/24 [00:00<00:00, 47.53it/s]\n"
          ]
        }
      ],
      "source": [
        "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
        "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "xxhZPqkNzZlh"
      },
      "outputs": [],
      "source": [
        "finetune_results_df = pd.DataFrame(finetune_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4thAK2BXzaj6",
        "outputId": "de45fd39-fc03-41a4-a1ad-d2414de57d96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4166666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
        "finetune_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegFM209mBk3"
      },
      "source": [
        "## Task 1: Vibe Checking the RAG Pipeline\n",
        "\n",
        "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzg0AA5krgR4"
      },
      "source": [
        "### Creating New Chunks\n",
        "\n",
        "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "KwQ2_LqNr0Tw"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 600,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIdxahHXpP-c"
      },
      "source": [
        "### Base Chain\n",
        "\n",
        "We'll start by constructing our base chain, which will use the untrained retrieval model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOsxIXpNpWC2"
      },
      "source": [
        "#### R - Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "azIGIKYfmNCT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
        "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-1nVZ0KpX5N"
      },
      "source": [
        "#### A - Augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "G10Fr-aKojeA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euq6RQEopZvD"
      },
      "source": [
        "#### G - Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "5-mfbbrypMHG"
      },
      "outputs": [],
      "source": [
        "rag_llm =  ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ2p4mnUpbYY"
      },
      "source": [
        "#### RAG - LCEL RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ssuR-LaboyGq"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "base_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "emm6WbB9pfKt",
        "outputId": "8257901e-5878-46be-c4da-21e9dc22ac5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An agent, in the context of AI, is a term that refers to AI systems that can act on your behalf. However, the term is considered vague and lacks a single, clear definition. There are different interpretations, such as AI systems that autonomously perform tasks (like a travel agent) or LLMs (large language models) that utilize tools to solve problems. The concept of \"agents\" is still evolving, and there is skepticism about their utility due to challenges like gullibility, where these systems may struggle to distinguish truth from fiction.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mUOrd0OBprAq",
        "outputId": "8aece8af-a7a2-4ae4-f49f-5d7ba21eb7e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OnfuFl59py7I",
        "outputId": "c6b48a08-f852-4b63-8549-050ae6aa15b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I do not know.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the laziest month for AI?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-NmqwHBDqTZ8",
        "outputId": "06c428dc-ae52-44f9-ab11-617ffc6682ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I do not know.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNS0UJAp3lC"
      },
      "source": [
        "### Fine-tuned Embedding Model\n",
        "\n",
        "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ihO7tP6mqATy"
      },
      "outputs": [],
      "source": [
        "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
        "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "1_cIFvWzqKGY"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "OJmRHJF2qNgj",
        "outputId": "fe6ce4b7-fab8-4e9f-f124-ef306d406875"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An \"Agent\" is a term that lacks a single, clear, and widely understood meaning in the context of AI. It generally refers to AI systems that are intended to act on behalf of users, but the specifics can vary widely. Some people think of agents as systems that can autonomously perform tasks, similar to a travel agent, while others view them as LLMs (Large Language Models) that utilize tools to solve problems. The term is often used without a clear definition, leading to confusion and skepticism about their utility, particularly due to issues like gullibility in AI systems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "EnK-c2ugqPPh",
        "outputId": "fdcf6fe6-e08a-44c7-bf74-e28967e6638d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "83hssg1AWozc",
        "outputId": "19a44069-1467-4978-8197-796e2c9649a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The context suggests that December might be considered the laziest month for AI, as it mentions the possibility that ChatGPT gets lazy in December due to its hidden system prompt including the current date and the observation that people provide less useful answers coming up to the holidays.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the laziest month for AI?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rsHmGeFbqRET",
        "outputId": "050c6f45-1b98-4dba-d6be-15727861d4ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I do not know.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgD8seY_I3W"
      },
      "source": [
        "####❓Question #2:\n",
        "\n",
        "Which LCEL RAG Chain do you think answered the questions better, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finetune_rag_chain was clearly able to perform much better by able to answer domain specific questions. It was able to do so because we finetuned it with Matryoshka loss funtion. It helps train the model with the most important embeddings.This type of finetuning is very useful in case of domain specific finetuning."
      ],
      "metadata": {
        "id": "hi-NOlqtvxzx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udgw06x0vqGa"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbq1sZArIx4"
      },
      "source": [
        "## Task 2: RAGAS Evaluation\n",
        "\n",
        "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n",
        "\n",
        "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "jq880DtHk9pX"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "!pip install -qU ragas==0.2.10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ragas import EvaluationDataset, evaluate, RunConfig\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n"
      ],
      "metadata": {
        "id": "d7UQshKgLFin"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build records: for each question, create an evaluation instance.\n",
        "records = []\n",
        "for qid, question in test_dataset[\"questions\"].items():\n",
        "    context_ids = test_dataset[\"relevant_contexts\"].get(qid, [])\n",
        "    ref_id = context_ids[0] if context_ids else None\n",
        "    reference_text = test_dataset[\"corpus\"].get(ref_id, \"\") if ref_id else \"\"\n",
        "\n",
        "    records.append({\n",
        "         \"id\": qid,\n",
        "         \"question\": question,            # Input question\n",
        "         \"reference\": reference_text,        # Expected context (ground truth)\n",
        "         \"retrieved_contexts\": [],           # To be filled by the chain\n",
        "         \"response\": \"\"                      # To be filled by the chain\n",
        "    })\n",
        "\n",
        "# Convert records to a DataFrame.\n",
        "df = pd.DataFrame(records)\n",
        "print(\"DataFrame:\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "NV284LS_QAVc",
        "outputId": "e55d74b0-511a-4827-ec87-de87c12f45f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame:\n",
            "                                      id  \\\n",
            "0   dc63318e-e94d-4b13-b24d-80b92806425f   \n",
            "1   96401b36-5881-41e8-9d53-dce58ff417e3   \n",
            "2   c2a9cf20-150c-498b-aea6-6e3c6ade1fb8   \n",
            "3   4d4c66ca-8f13-406e-9e93-1d71a74dd99e   \n",
            "4   e7d8f869-2449-4be4-971b-b620a26dc390   \n",
            "5   b8223543-f174-4804-aa65-5f71eb328ff1   \n",
            "6   5b30709c-61f8-4424-9205-43cb616b0b88   \n",
            "7   c500a394-57b1-470c-abae-6f961bcda8e8   \n",
            "8   991032a7-bd9a-4ba2-a5a7-99d9982a2adc   \n",
            "9   cd3ed50c-2c32-41e0-a426-0a7eef323327   \n",
            "10  3477139b-4d18-4303-a9ce-3279458867a2   \n",
            "11  da8ed472-c9f8-47eb-86a5-ad18b9ebafbd   \n",
            "12  8ab817b8-3186-4c34-8ac0-b57777366842   \n",
            "13  78db9829-598a-4f9b-9522-ce1962164aa0   \n",
            "14  1e268502-7b59-4a7f-aafc-aa187f87188d   \n",
            "15  d1422cf9-a1ec-451b-843b-8e2300f01b71   \n",
            "16  e025bd80-699a-495c-95c8-2cdecef51f3f   \n",
            "17  7ecc4a8e-8ddb-444f-8cbb-77fbffd0184d   \n",
            "18  814ce667-931c-4693-9f25-927d1ecb1cd6   \n",
            "19  aaf8e814-f746-4eff-bcee-904d7d7044e5   \n",
            "20  d5515c7d-a821-44cd-9059-765df2823a5a   \n",
            "21  3918e5ee-18f0-4215-bf79-4ce2ca12d662   \n",
            "22  caa721f7-94fd-4451-9c5b-5e543df45e05   \n",
            "23  5274c865-f16c-46f0-a8e2-548aa940bc98   \n",
            "\n",
            "                                             question  \\\n",
            "0   Sample question 1 related to Stuff we figured ...   \n",
            "1   Sample question 2 related to Stuff we figured ...   \n",
            "2   Sample question 1 related to Stuff we figured ...   \n",
            "3   Sample question 2 related to Stuff we figured ...   \n",
            "4   Sample question 1 related to Stuff we figured ...   \n",
            "5   Sample question 2 related to Stuff we figured ...   \n",
            "6   Sample question 1 related to Stuff we figured ...   \n",
            "7   Sample question 2 related to Stuff we figured ...   \n",
            "8   Sample question 1 related to Stuff we figured ...   \n",
            "9   Sample question 2 related to Stuff we figured ...   \n",
            "10  Sample question 1 related to Stuff we figured ...   \n",
            "11  Sample question 2 related to Stuff we figured ...   \n",
            "12  Sample question 1 related to Stuff we figured ...   \n",
            "13  Sample question 2 related to Stuff we figured ...   \n",
            "14  Sample question 1 related to Stuff we figured ...   \n",
            "15  Sample question 2 related to Stuff we figured ...   \n",
            "16  Sample question 1 related to Stuff we figured ...   \n",
            "17  Sample question 2 related to Stuff we figured ...   \n",
            "18  Sample question 1 related to Stuff we figured ...   \n",
            "19  Sample question 2 related to Stuff we figured ...   \n",
            "20  Sample question 1 related to Stuff we figured ...   \n",
            "21  Sample question 2 related to Stuff we figured ...   \n",
            "22  Sample question 1 related to Stuff we figured ...   \n",
            "23  Sample question 2 related to Stuff we figured ...   \n",
            "\n",
            "                                            reference retrieved_contexts  \\\n",
            "0   A lot of people are excited about AI agents—an...                 []   \n",
            "1   A lot of people are excited about AI agents—an...                 []   \n",
            "2   If you think about what they do, this isn’t su...                 []   \n",
            "3   If you think about what they do, this isn’t su...                 []   \n",
            "4   Except... you can run generated code to see if...                 []   \n",
            "5   Except... you can run generated code to see if...                 []   \n",
            "6   On the other hand, as software engineers we ar...                 []   \n",
            "7   On the other hand, as software engineers we ar...                 []   \n",
            "8   Just this week, the New York Times launched a ...                 []   \n",
            "9   Just this week, the New York Times launched a ...                 []   \n",
            "10  Law is not ethics. Is it OK to train models on...                 []   \n",
            "11  Law is not ethics. Is it OK to train models on...                 []   \n",
            "12  The top five: ai (342), generativeai (300), ll...                 []   \n",
            "13  The top five: ai (342), generativeai (300), ll...                 []   \n",
            "14  Embeddings: What they are and why they matter\\...                 []   \n",
            "15  Embeddings: What they are and why they matter\\...                 []   \n",
            "16  Now add a walrus: Prompt engineering in DALL-E...                 []   \n",
            "17  Now add a walrus: Prompt engineering in DALL-E...                 []   \n",
            "18  I also gave a bunch of talks and podcast appea...                 []   \n",
            "19  I also gave a bunch of talks and podcast appea...                 []   \n",
            "20  Industry’s Tardy Response to the AI Prompt Inj...                 []   \n",
            "21  Industry’s Tardy Response to the AI Prompt Inj...                 []   \n",
            "22  ai\\n            1105\\n\\n\\n            generati...                 []   \n",
            "23  ai\\n            1105\\n\\n\\n            generati...                 []   \n",
            "\n",
            "   response  \n",
            "0            \n",
            "1            \n",
            "2            \n",
            "3            \n",
            "4            \n",
            "5            \n",
            "6            \n",
            "7            \n",
            "8            \n",
            "9            \n",
            "10           \n",
            "11           \n",
            "12           \n",
            "13           \n",
            "14           \n",
            "15           \n",
            "16           \n",
            "17           \n",
            "18           \n",
            "19           \n",
            "20           \n",
            "21           \n",
            "22           \n",
            "23           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_records = []\n",
        "for idx, row in df.iterrows():\n",
        "    # Convert the row to a dict.\n",
        "    input_item = row.to_dict()\n",
        "    # Ensure the input has the key \"question\"\n",
        "    if \"question\" not in input_item and \"user_input\" in input_item:\n",
        "        input_item[\"question\"] = input_item[\"user_input\"]\n",
        "\n",
        "    # Run your chain using the .invoke() method.\n",
        "    output = finetune_rag_chain.invoke(input_item)\n",
        "\n",
        "    # Update the record with generated outputs.\n",
        "    input_item[\"response\"] = output.get(\"response\", \"\")\n",
        "\n",
        "    # Process the \"context\" output to ensure we have a list of strings.\n",
        "    ctx = output.get(\"context\", \"\")\n",
        "    if isinstance(ctx, list):\n",
        "        # If it's a list, convert each Document to its text if necessary.\n",
        "        processed_ctx = []\n",
        "        for item in ctx:\n",
        "            if hasattr(item, \"page_content\"):\n",
        "                processed_ctx.append(item.page_content)\n",
        "            else:\n",
        "                processed_ctx.append(str(item))\n",
        "    else:\n",
        "        # If a single value, convert it appropriately.\n",
        "        if hasattr(ctx, \"page_content\"):\n",
        "            processed_ctx = [ctx.page_content]\n",
        "        else:\n",
        "            processed_ctx = [str(ctx)]\n",
        "\n",
        "    input_item[\"retrieved_contexts\"] = processed_ctx\n",
        "    generated_records.append(input_item)\n",
        "\n",
        "# Create a new DataFrame with the generated outputs.\n",
        "df_generated = pd.DataFrame(generated_records)\n",
        "print(\"DataFrame with Generated Outputs:\")\n",
        "print(df_generated)"
      ],
      "metadata": {
        "id": "f-T1wwFIYI2y",
        "outputId": "4e701757-97c5-466a-ccef-268039b8546d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with Generated Outputs:\n",
            "                                      id  \\\n",
            "0   dc63318e-e94d-4b13-b24d-80b92806425f   \n",
            "1   96401b36-5881-41e8-9d53-dce58ff417e3   \n",
            "2   c2a9cf20-150c-498b-aea6-6e3c6ade1fb8   \n",
            "3   4d4c66ca-8f13-406e-9e93-1d71a74dd99e   \n",
            "4   e7d8f869-2449-4be4-971b-b620a26dc390   \n",
            "5   b8223543-f174-4804-aa65-5f71eb328ff1   \n",
            "6   5b30709c-61f8-4424-9205-43cb616b0b88   \n",
            "7   c500a394-57b1-470c-abae-6f961bcda8e8   \n",
            "8   991032a7-bd9a-4ba2-a5a7-99d9982a2adc   \n",
            "9   cd3ed50c-2c32-41e0-a426-0a7eef323327   \n",
            "10  3477139b-4d18-4303-a9ce-3279458867a2   \n",
            "11  da8ed472-c9f8-47eb-86a5-ad18b9ebafbd   \n",
            "12  8ab817b8-3186-4c34-8ac0-b57777366842   \n",
            "13  78db9829-598a-4f9b-9522-ce1962164aa0   \n",
            "14  1e268502-7b59-4a7f-aafc-aa187f87188d   \n",
            "15  d1422cf9-a1ec-451b-843b-8e2300f01b71   \n",
            "16  e025bd80-699a-495c-95c8-2cdecef51f3f   \n",
            "17  7ecc4a8e-8ddb-444f-8cbb-77fbffd0184d   \n",
            "18  814ce667-931c-4693-9f25-927d1ecb1cd6   \n",
            "19  aaf8e814-f746-4eff-bcee-904d7d7044e5   \n",
            "20  d5515c7d-a821-44cd-9059-765df2823a5a   \n",
            "21  3918e5ee-18f0-4215-bf79-4ce2ca12d662   \n",
            "22  caa721f7-94fd-4451-9c5b-5e543df45e05   \n",
            "23  5274c865-f16c-46f0-a8e2-548aa940bc98   \n",
            "\n",
            "                                             question  \\\n",
            "0   Sample question 1 related to Stuff we figured ...   \n",
            "1   Sample question 2 related to Stuff we figured ...   \n",
            "2   Sample question 1 related to Stuff we figured ...   \n",
            "3   Sample question 2 related to Stuff we figured ...   \n",
            "4   Sample question 1 related to Stuff we figured ...   \n",
            "5   Sample question 2 related to Stuff we figured ...   \n",
            "6   Sample question 1 related to Stuff we figured ...   \n",
            "7   Sample question 2 related to Stuff we figured ...   \n",
            "8   Sample question 1 related to Stuff we figured ...   \n",
            "9   Sample question 2 related to Stuff we figured ...   \n",
            "10  Sample question 1 related to Stuff we figured ...   \n",
            "11  Sample question 2 related to Stuff we figured ...   \n",
            "12  Sample question 1 related to Stuff we figured ...   \n",
            "13  Sample question 2 related to Stuff we figured ...   \n",
            "14  Sample question 1 related to Stuff we figured ...   \n",
            "15  Sample question 2 related to Stuff we figured ...   \n",
            "16  Sample question 1 related to Stuff we figured ...   \n",
            "17  Sample question 2 related to Stuff we figured ...   \n",
            "18  Sample question 1 related to Stuff we figured ...   \n",
            "19  Sample question 2 related to Stuff we figured ...   \n",
            "20  Sample question 1 related to Stuff we figured ...   \n",
            "21  Sample question 2 related to Stuff we figured ...   \n",
            "22  Sample question 1 related to Stuff we figured ...   \n",
            "23  Sample question 2 related to Stuff we figured ...   \n",
            "\n",
            "                                            reference  \\\n",
            "0   A lot of people are excited about AI agents—an...   \n",
            "1   A lot of people are excited about AI agents—an...   \n",
            "2   If you think about what they do, this isn’t su...   \n",
            "3   If you think about what they do, this isn’t su...   \n",
            "4   Except... you can run generated code to see if...   \n",
            "5   Except... you can run generated code to see if...   \n",
            "6   On the other hand, as software engineers we ar...   \n",
            "7   On the other hand, as software engineers we ar...   \n",
            "8   Just this week, the New York Times launched a ...   \n",
            "9   Just this week, the New York Times launched a ...   \n",
            "10  Law is not ethics. Is it OK to train models on...   \n",
            "11  Law is not ethics. Is it OK to train models on...   \n",
            "12  The top five: ai (342), generativeai (300), ll...   \n",
            "13  The top five: ai (342), generativeai (300), ll...   \n",
            "14  Embeddings: What they are and why they matter\\...   \n",
            "15  Embeddings: What they are and why they matter\\...   \n",
            "16  Now add a walrus: Prompt engineering in DALL-E...   \n",
            "17  Now add a walrus: Prompt engineering in DALL-E...   \n",
            "18  I also gave a bunch of talks and podcast appea...   \n",
            "19  I also gave a bunch of talks and podcast appea...   \n",
            "20  Industry’s Tardy Response to the AI Prompt Inj...   \n",
            "21  Industry’s Tardy Response to the AI Prompt Inj...   \n",
            "22  ai\\n            1105\\n\\n\\n            generati...   \n",
            "23  ai\\n            1105\\n\\n\\n            generati...   \n",
            "\n",
            "                                   retrieved_contexts  \\\n",
            "0   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "1   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "2   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "3   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "4   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "5   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "6   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "7   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "8   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "9   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "10  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "11  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "12  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "13  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "14  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "15  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "16  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "17  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "18  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "19  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "20  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "21  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "22  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "23  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
            "\n",
            "                                             response  \n",
            "0   The document discusses various insights and de...  \n",
            "1                                      I do not know.  \n",
            "2   The document discusses various insights and de...  \n",
            "3                                      I do not know.  \n",
            "4   The document discusses various insights and de...  \n",
            "5                                      I do not know.  \n",
            "6   The document discusses various insights and de...  \n",
            "7                                      I do not know.  \n",
            "8   The document discusses various insights and de...  \n",
            "9                                      I do not know.  \n",
            "10  The document discusses various insights and de...  \n",
            "11                                     I do not know.  \n",
            "12  The document discusses various insights and de...  \n",
            "13                                     I do not know.  \n",
            "14  The document discusses various insights and de...  \n",
            "15                                     I do not know.  \n",
            "16  The document discusses various insights and de...  \n",
            "17                                     I do not know.  \n",
            "18  The document discusses various insights and de...  \n",
            "19                                     I do not know.  \n",
            "20  The document discusses various insights and de...  \n",
            "21                                     I do not know.  \n",
            "22  The document discusses various insights and de...  \n",
            "23                                     I do not know.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMAdapter:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def __call__(self, input_data):\n",
        "        return self.llm(input_data)\n",
        "\n",
        "    def set_run_config(self, run_config):\n",
        "        # Return self or optionally store the run_config if needed.\n",
        "        return self\n",
        "\n",
        "# Wrap your evaluator_llm:\n",
        "adapter_llm = LLMAdapter(LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\")))"
      ],
      "metadata": {
        "id": "hLlx2VhapvE2"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate, RunConfig\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "\n",
        "# Patch set_run_config onto LangchainLLMWrapper\n",
        "def set_run_config(self, run_config):\n",
        "    return self\n",
        "\n",
        "LangchainLLMWrapper.set_run_config = set_run_config\n",
        "\n",
        "# Create your evaluator_llm\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "\n",
        "# Set up your run config and metrics (assumed already defined)\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "metrics = [\n",
        "\n",
        "    Faithfulness(),\n",
        "    FactualCorrectness(),\n",
        "    ResponseRelevancy(),\n",
        "    ContextEntityRecall(),\n",
        "    NoiseSensitivity()\n",
        "]"
      ],
      "metadata": {
        "id": "qSTgbmhkw3oJ"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_generated = df_generated.rename(columns={\"question\": \"user_input\"})\n",
        "\n",
        "# Optionally, verify the columns:\n",
        "print(\"Columns in df_generated:\", df_generated.columns)\n",
        "\n",
        "# Convert df_generated (a DataFrame) into an EvaluationDataset\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(df_generated)\n",
        "\n",
        "# Now call evaluate() with the evaluation_dataset, not the DataFrame.\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=metrics,\n",
        "    llm=evaluator_llm,  # your patched/wrapped LLM\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "m4Md1T5A1TYK",
        "outputId": "7f24f27e-29a0-4446-eff5-148a29f5c522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "4fa678137e0849a3a5ad862f1daaa753",
            "a432bac888f84275b1b6f00d4a0668eb",
            "c8b63bd98c9e451f902bf5a3a9dd6ce9",
            "d5d71193ee7b48968012230da407c2d7",
            "231620d53a5449d7b9a0c03750c8c95b",
            "a1c517f523e64fb9b7e808e4202202d1",
            "bc833b8309c94dce83581b1d9f6f543c",
            "a9773af0f50341ef85874e973c23cbbc",
            "0db0d34a99c84e95b8254f461f5eb3e0",
            "8375cfdad22d430a85b1646386019215",
            "7534b1481e684d57acf70641c67caa13"
          ]
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in df_generated: Index(['id', 'user_input', 'reference', 'retrieved_contexts', 'response'], dtype='object')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fa678137e0849a3a5ad862f1daaa753"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ragas.executor:Exception raised in Job[16]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[6]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[26]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[36]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[46]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[56]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[66]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[76]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[86]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[96]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[106]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "ERROR:ragas.executor:Exception raised in Job[74]: TimeoutError()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "{'faithfulness': 0.4815, 'factual_correctness': 0.1438, 'answer_relevancy': 0.3828, 'context_entity_recall': 0.1535, 'noise_sensitivity_relevant': 0.2001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall the finetune_rag_chain has performed not very well. But notably compared to all other metrics, 'faithfulness' is the highest. I can infer that from the 'faithfulness' score ('faithfulness' indicates how much your models is hallucinating ) that when it comes to domain related questions, our model performs relatively better compared to all other metrics."
      ],
      "metadata": {
        "id": "YoWZdb-C6xKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UTXWiISf8Bhs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60615941472b43ec9d0941fca22ee33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08dcde29f0d848bb82f8f244115f4896",
              "IPY_MODEL_d8156dd3afdb4a4eb74b163450541111",
              "IPY_MODEL_7be2866ce8f740f287405fa88b71cdd1"
            ],
            "layout": "IPY_MODEL_88680d45b33b482a8ecd2f66ba1b3d92"
          }
        },
        "08dcde29f0d848bb82f8f244115f4896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f6458a5baa346ea8829dd72290efb12",
            "placeholder": "​",
            "style": "IPY_MODEL_8b2d465a18724cfabb02aa288963edc6",
            "value": "Computing widget examples:   0%"
          }
        },
        "d8156dd3afdb4a4eb74b163450541111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37132f1b31e94ac1ab4887f10094275a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_182106faf6824da1b2125de9d1118698",
            "value": 1
          }
        },
        "7be2866ce8f740f287405fa88b71cdd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c93ea3624814d479d1bf4a849c8672e",
            "placeholder": "​",
            "style": "IPY_MODEL_3490c5d6b98c4fb8bfc5e7bfa759b81b",
            "value": " 0/1 [00:00&lt;?, ?example/s]"
          }
        },
        "88680d45b33b482a8ecd2f66ba1b3d92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4f6458a5baa346ea8829dd72290efb12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2d465a18724cfabb02aa288963edc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37132f1b31e94ac1ab4887f10094275a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "182106faf6824da1b2125de9d1118698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c93ea3624814d479d1bf4a849c8672e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3490c5d6b98c4fb8bfc5e7bfa759b81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f41757f2cb3408ba71ae1bcfee09e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_1f70c456bc984fbeb805a2756eb22484"
          }
        },
        "0d56170b06a14c1894d1245d8f72704e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f69ef1f32e44e2aaf16db3304e26619",
            "placeholder": "​",
            "style": "IPY_MODEL_fafb3043d8454e9a94f64eefc6929d08",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "00dbf921c1fc4c0fa1d7724b6092d4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5039fa208fff45c3a44734c78778f7e5",
            "placeholder": "​",
            "style": "IPY_MODEL_b351b8dd4c194f8281285144713fbb6d",
            "value": ""
          }
        },
        "e3f63997fedd4815986a2c863d2d7a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_bb0711bebea746c496d61fbfb566aa92",
            "style": "IPY_MODEL_715f3b6375e6427e862101a49b5c540e",
            "value": true
          }
        },
        "5efe52f9b5cb48e7810b3d82e50f0c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d2bc8f44b3ae47b193bd1474908f1f84",
            "style": "IPY_MODEL_65ca5f4fa73c4468a9242a1ccafec17c",
            "tooltip": ""
          }
        },
        "4d95e99cfc784545a0045447138ffd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_525b2b353f214f22a096245e8304caab",
            "placeholder": "​",
            "style": "IPY_MODEL_a191e28db0234462b789a6bd8e94ad07",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "1f70c456bc984fbeb805a2756eb22484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "2f69ef1f32e44e2aaf16db3304e26619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fafb3043d8454e9a94f64eefc6929d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5039fa208fff45c3a44734c78778f7e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b351b8dd4c194f8281285144713fbb6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb0711bebea746c496d61fbfb566aa92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715f3b6375e6427e862101a49b5c540e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2bc8f44b3ae47b193bd1474908f1f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65ca5f4fa73c4468a9242a1ccafec17c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "525b2b353f214f22a096245e8304caab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a191e28db0234462b789a6bd8e94ad07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa7d900c1aca46b1974d0b20568ace00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9d29326d601444f92665e88c832763f",
            "placeholder": "​",
            "style": "IPY_MODEL_89b72fb1910a4cb0b52fc600399dcae5",
            "value": "Connecting..."
          }
        },
        "b9d29326d601444f92665e88c832763f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b72fb1910a4cb0b52fc600399dcae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fa678137e0849a3a5ad862f1daaa753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a432bac888f84275b1b6f00d4a0668eb",
              "IPY_MODEL_c8b63bd98c9e451f902bf5a3a9dd6ce9",
              "IPY_MODEL_d5d71193ee7b48968012230da407c2d7"
            ],
            "layout": "IPY_MODEL_231620d53a5449d7b9a0c03750c8c95b"
          }
        },
        "a432bac888f84275b1b6f00d4a0668eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c517f523e64fb9b7e808e4202202d1",
            "placeholder": "​",
            "style": "IPY_MODEL_bc833b8309c94dce83581b1d9f6f543c",
            "value": "Evaluating: 100%"
          }
        },
        "c8b63bd98c9e451f902bf5a3a9dd6ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9773af0f50341ef85874e973c23cbbc",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0db0d34a99c84e95b8254f461f5eb3e0",
            "value": 120
          }
        },
        "d5d71193ee7b48968012230da407c2d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8375cfdad22d430a85b1646386019215",
            "placeholder": "​",
            "style": "IPY_MODEL_7534b1481e684d57acf70641c67caa13",
            "value": " 120/120 [09:30&lt;00:00, 26.37s/it]"
          }
        },
        "231620d53a5449d7b9a0c03750c8c95b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c517f523e64fb9b7e808e4202202d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc833b8309c94dce83581b1d9f6f543c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9773af0f50341ef85874e973c23cbbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0db0d34a99c84e95b8254f461f5eb3e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8375cfdad22d430a85b1646386019215": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7534b1481e684d57acf70641c67caa13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}